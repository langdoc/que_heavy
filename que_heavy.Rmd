---
title: "Gibraltar code-switching data to R"
author: "Niko Partanen"
date: "2 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
# library(devtools)
# install_github("langdoc/FRelan")
library(FRelan)
library(stringr)
```

This text is connected to Eugenio Gorio's code-switching data, but possibly could be used also with other similar datasets.

I first though to write this directly into function, but there are some things in the ELAN file structure which need some more thinking. I haven't usually encountered files with this many time aligned items, and this means it is necessary to join the file with those. This isn't any problem, but needs bit different approach than what I have usually had. It also illustrates well why it is so difficult to have a general solution that fits for every situation! Each project has very different ELAN file structures, and these are usually very well justified by the demands the reserach question has and so forth.

However, I also think that the majority of the corpora can be expressed in a tidy data frame with one token per row which has relatively similar structure across the corpora.

I was thinking of having both good and bad example so that one could easily illustrate how the reading can fail with different issues in the ELAN files. Now I have only good example. No file is yet part of the repository, but in principle there could be an almost empty anonymized dummy file which could be used to illustrate the logic behind the parsing strategy.

```{r}

elan_file <- "data/example_good.eaf"

```

Here we get one by one different tiers by their linguistic types

```{r}

def <- read_tier(elan_file, linguistic_type = "default-lt")
words <- read_tier(elan_file, linguistic_type = "Words")
language <- read_tier(elan_file, linguistic_type = "Language")
syntax <- read_tier(elan_file, linguistic_type = "Syntax")

```

Just a note: **this is already something one can easily work with**. For example, in these data frames there is already all information needed to look into frequencies of different kinds of annotations, for example.

```{r}

library(ggplot2)
ggplot(data = language,
       aes(x = content)) +
        geom_bar()

```

Anyway soon one would be interested about things like: which are those tokens tagged as *ita*? For this, more elaborate data frame is needed.

After receiving those (one can check how they look for example with `View(syntax)` ) we need to join these to one another to build one large data frame, and this is easily achieved by following the logic already present in the ELAN file. So if one looks the  way the ID's are constructed there, the tier always has annotation_ref attributes which then again are annotation_id's of the tier above.

I usually look in this point which is the elements contains most elements, since often that is the item we want to have on one row. As far as I see, in order to follow tidy data principles with linguistic data, the best possibility is to have something like one token per row, although in some cases other ways of doing it are certainly also OK.

This time the file structure demands a bit different approach, since some of the elements are not connected through annotation id's but the time codes. There is a function  `read_timeslots()` which can be used to access the timeslots in the file.

In this case def, words and syntax have the time slots.

The idea here is that we use left_join function to put the data frames together but we can control the exact way how they join by renaming the columns and selecting or unselecting some of them before the join is done. For example, the content is renamed utterance and so on.

```{r}
timeslots <- read_timeslots(elan_file)
```

I find this step often bit hard to understand, but basically we join the timeslots to the previous data frame twice, because with the first join we get the start time, and with the second one the end time.

```{r}

add_timeslots <- function(df){
        df %>%  left_join(timeslots %>% rename(time_slot_1 = time_slot_id, time_start = time_value)) %>%
                left_join(timeslots %>% rename(time_slot_2 = time_slot_id, time_end = time_value))
}

def <- add_timeslots(def)
words <- add_timeslots(words)
syntax <- add_timeslots(syntax)

```

Now those data frames have the actual start and end times per item. It seems that the `def` and `syntax` have the start and end times for each row, but the `words` does not have those always. I need to figure out why it is like that, probably because of the linguistic types as they are? 

```{r}

library(tidytext)

def
words

# As far as I see the language tier doesn't need those variables, so I remove them to keep it more simple
# When done this way, the join is controlled by the connecting id only.

syntax_lang <- left_join(syntax %>% 
                  rename(join_id = annot_id,
                         syntax = content) %>%
                  select(-ref_id, -tier_id), 
          language %>% 
                  rename(join_id = ref_id,
                         lang = content) %>%
                            select(-time_slot_1, -time_slot_2, -type, -tier_id, -annot_id, -participant),
          by = "join_id") %>% select(syntax, lang, time_start, time_end, join_id)

```

So now we have one data frame which contains both syntax and language tiers.

To deal with the words, there seems to be a pattern that when the `time_start` occurs, in a row before that there is a `time_end` value. So we can detect each unique `time_start` and treat it as a one span, and with this information add a variable `segment`. The first and last row of the segment carry information about the beginning and time of that span. 

```{r}
words <- words %>% mutate(segment = cumsum(grepl("\\d", as.character(time_start)))) %>%
        group_by(segment) %>% mutate(time_start = first(time_start),
                                     time_end = last(time_end)) %>%
        ungroup()

words
```

Now all tiers are in data frames and we have as complete time information about them as possible. It should be easy to join now the words with the utterances they belong to.

```{r}

words <- words %>% mutate(word_id = 1:n()) %>% rename(token = content)
def <- def %>% rename(utterance = content)

words %>% filter(str_detect(token, "army"))
def %>% filter(str_detect(utterance, "army"))

words_start <- words %>% filter(time_start %in% def$time_start)
words_end <- words %>% filter(time_start %in% def$time_end)
words_missing <- words %>% filter(! word_id %in% c(words_start$word_id, words_end$word_id))

def_join <- def %>% select(-annot_id, -ref_id, -tier_id, -type, -time_slot_1, -time_slot_2, -participant)

rbind(left_join(words_start, def_join %>% select(-time_end)) %>% 
              left_join(words_end %>% select(token, time_end)), 
      words_missing %>% mutate(utterance = NA)) %>%
        arrange(word_id) %>% 
        select(token, utterance, participant, segment, word_id, time_start, time_end) %>% 
        mutate(time_start = plyr::round_any(time_start, 10),
                                         time_end = plyr::round_any(time_end, 10)) %>%
        left_join(syntax_lang %>% 
                          mutate(time_start = plyr::round_any(time_start, 10),
                                         time_end = plyr::round_any(time_end, 10)))

```

Currently the problem is that in syntax and language annotations the time spans are smaller than on higher level, and this makes joining them as easily as I thought bit difficult. It should be doable anyway, as this is systematic.

```{r}
words
syntax_lang

```


### Notes

- I used to do lots of things with `plyr` package, but now that is not part of the so-called `tidyverse`, and apparently one can use package `purrr` to do similar loop like operations to data frames in more elegant manner
- `FRelan` package still uses a lot `XML` package, and should probably be updated to work with `xml2`
- There are probably some new and better ways to rewrite the whole `read_tier` and `read_eaf` functions, but I think we can't get anywhere from this reading tiers as data frames and joining them by the logic which is present in the ELAN file. Or maybe there is some smarter way, but I can't imagine how it exactly should go! 
